---
layout: post
title:  "From scratch: reverse-mode automatic differentiation (in Python)"
date:   2020-06-11 00:00:00 +0000
date_edited: 2021-01-11 00:00:00 +0000
categories:
comments: true
nolink: false
---
{%- include mathjax.html -%}

Automatic differentiation is the foundation upon which deep learning frameworks lie. Deep learning models are typically trained using gradient based techniques, and autodiff makes it easy to get gradients, even from enormous, complex models. 'Reverse-mode autodiff' is the autodiff method used by most deep learning frameworks, due to its efficiency and accuracy. 

Let's:

- Look at how reverse-mode autodiff works.
- Create a minimal autodiff framework in Python.

The small autodiff framework will deal with scalars. We will later use NumPy to vectorise it.

Note on terminology: from now on 'autodiff' will refer to 'reverse-mode autodiff'. 'Gradient' is used loosely, but in this context generally means 'first order partial derivative'.


## How does autodiff work?

Let's start with an example.

```python
a = 4
b = 3
c = a + b  # = 4 + 3 = 7
d = a * c  # = 4 * 7 = 28
```

**Q1**: What is the gradient of $d$ with respect to $a$, i.e. $\frac{\partial{d}}{\partial{a}}$? (Go ahead and try this!)

#### Solving the 'traditional' way:

There's many ways to solve **Q1**, but let's use the product rule, i.e. if $ y = x_1x_2 $ then $y' = x_1'x_2 + x_1x_2'$. 

$$ d = a * c $$

$$ \frac{\partial{d}}{\partial{a}} = \frac{\partial{a}}{\partial{a}} * c + a * \frac{\partial{c}}{\partial{a}} $$

$$ \frac{\partial{d}}{\partial{a}} = c + a * \frac{\partial{c}}{\partial{a}} $$

$$ \frac{\partial{d}}{\partial{a}} = (a + b) + a * \frac{\partial{(a + b)}}{\partial{a}} $$

$$ \frac{\partial{d}}{\partial{a}} = a + b + a * (\frac{\partial{a}}{\partial{a}} + \frac{\partial{b}}{\partial{a}})$$

$$ \frac{\partial{d}}{\partial{a}} = a + b + a * (1 + 0)$$

$$ \frac{\partial{d}}{\partial{a}} = a + b + a $$

$$ \frac{\partial{d}}{\partial{a}} = 2a + b $$

$$ \frac{\partial{d}}{\partial{a}} = 2*4 + 3 = 11$$

Phew... and if you wanted to know $\frac{\partial{d}}{\partial{b}}$ you'd have to carry out the process again.

#### Solving the autodiff way

We'll now look at the autodiff way to solve **Q1**. Here is a figure:

<p align="center">
<img 
    src="/assets/posts/autodiff/abcd.png" 
    alt="The system as a graph."
/>
</p>


On the left we see the system represented as a graph. Each variable is a node; e.g. $d$ is the topmost node, and $a$ and $b$ are leaf nodes at the bottom.

On the right we see the system from autodiff's point of view. Let's call the values on the graph edges *local derivatives*. By using local derivatives and simple rules, we will be able to compute the derivatives that we want.

Here is the answer to **Q1**, calculated the autodiff way. Can you see how it relates to the figure?

$$ \frac{\partial{d}}{\partial{a}} = \frac{\partial{\bar{d}}}{\partial{a}} + \frac{\partial{\bar{d}}}{\partial{c}} * \frac{\partial{\bar{c}}}{\partial{a}} $$

$$ \frac{\partial{d}}{\partial{a}} = c + a * 1 $$

$$ \frac{\partial{d}}{\partial{a}} = a + b + a $$

$$ \frac{\partial{d}}{\partial{a}} = 2a + b $$

$$ \frac{\partial{d}}{\partial{a}} = 11 $$

We get this answer from the graph by finding the path from $d$ to $a$ (not going against the dotted arrows), and then applying the following rules:

- Multiply the edges of a path.
- Add together the different path.

The first path is straight from $d$ to $a$, which gives us the $\frac{\partial{\bar{d}}}{\partial{a}}$ term. The second path is from $d$ to $c$ to $a$, which gives us the term $\frac{\partial{\bar{d}}}{\partial{c}} * \frac{\partial{\bar{c}}}{\partial{a}}$.

Our autodiff implementation will go down the graph and compute the derivative of $d$ with respect to every sub-node, rather than just computing it for a particular node, as we have just done with $d$. Notice that we could compute the gradient of $d$ with respect to $c$, and $b$, without much more work.


### Local derivatives

We saw 'local derivatives' on the graph edges above, written in the form: $ \frac{\partial \bar{y} }{\partial x}$.

The bar is to convey a simpler kind of differentiation.

In general: to get a local derivative, treat the variables going into a node as not being functions of other variables.

Note the distinction between local derivatives and partial derivatives: a partial derivative takes the whole graph into account (still holding the other variables as constant), whereas a local derivative doesn't.

For example, recall that $d = a * c$. Then compare $ \frac{\partial{d}}{\partial{a}} = 2a + b $, to $ \frac{\partial \bar{d} }{\partial a} = c $. The local derivative $\frac{\partial \bar{d} }{\partial a}  = c$, is obtained by treating $c$ as a constant before differentiating the expression for $d$.

**Local derivatives make our lives easier.**

It is often easy to define the local derivatives of simple functions, and 
adding functions to the autodiff framework is easy if you know the local derivatives. E.g.

Addition: $n = a + b$.
The local derivatives are: $\frac{\partial \bar{n} }{\partial a}  = 1$ and $\frac{\partial \bar{n} }{\partial b}  = 1$.

Multiplication: $n = a * b$.
The local derivatives are: $\frac{\partial \bar{n} }{\partial a}  = b$ and $\frac{\partial \bar{n} }{\partial b}  = a$.

## Let's create the framework

#### Description of the implementation.

A variable (or node) contains two pieces of data:

- `value` - the value of the variable.
- `local_gradients` - the variable's children & corresponding 'local derivatives'.
    
The function `get_gradients` uses the variables' `local_gradients` data to go through the 
graph recursively, computing the gradients. 
It uses the rules we saw above:

- Multiply the edges of a path (into a variable).
- Add together the different paths (that lead to a variable).

### Implementing just enough to solve the example above:

```python
from collections import defaultdict

class Variable:
    def __init__(self, value, local_gradients=[]):
        self.value = value
        self.local_gradients = local_gradients
    
def add(a, b):
    "Create the variable that results from adding two variables."
    value = a.value + b.value    
    local_gradients = (
        (a, 1),  # the local derivative with respect to a is 1
        (b, 1)   # the local derivative with respect to b is 1
    )
    return Variable(value, local_gradients)

def mul(a, b):
    "Create the variable that results from multiplying two variables."
    value = a.value * b.value
    local_gradients = (
        (a, b.value), # the local derivative with respect to a is b.value
        (b, a.value)  # the local derivative with respect to b is a.value
    )
    return Variable(value, local_gradients)

def get_gradients(variable):
    """ Compute the first derivatives of `variable` 
    with respect to its sub/child variables.
    
    The graph is traversed recursively using `local_gradients`,
    which contain references to a variable's
    child variables.
    I.e.    
    `local_gradients` contains references to child variables,
    which have their own `local_gradients`,
    which contain references to child variables,
    which have their own `local_gradients`,
    which contain references to child variables,
    etc.
    
    The gradient of a child variable is computed by:
    - For each path from `variable` to the child variable,
    multiply the edges of the path (this is `path_value`).
    - Sum the path values.
    This gives the first derivative of `variable`,
    with respect to the child variable.
    
    """
    gradients = defaultdict(lambda: 0)
    
    def compute_gradients(variable, path_value):
        for child_variable, local_gradient in variable.local_gradients:
            # multiply path edges:
            child_path_value = path_value * local_gradient
            # add the paths:
            gradients[child_variable] += child_path_value
            # recurse through graph:
            compute_gradients(child_variable, child_path_value)
    
    compute_gradients(variable, path_value=1)  
    # (path_value=1 is from `variable` differentiated by itself.)
    return dict(gradients)
```

#### Solving the example above:


```python
a = Variable(4)
b = Variable(3)
c = add(a, b) # = 4 + 3 = 7
d = mul(a, c) # = 4 * 7 = 28

gradients = get_gradients(d)

print('d.value =', d.value)
print("The partial derivative of d with respect to a =", gradients[a])
```

```
d.value = 28
The partial derivative of d with respect to a = 11
``` 

**Success!**

Note we also get gradients for the other nodes:


```python
print('gradients[b] =', gradients[b])
print('gradients[c] =', gradients[c])
```

```
gradients[b] = 4
gradients[c] = 4
``` 

Let's take a look at the `grad` values (the local derivatives):


```python
print('dict(d.local_gradients)[a] =', dict(d.local_gradients)[a])
print('dict(d.local_gradients)[c] =', dict(d.local_gradients)[c])
print('dict(c.local_gradients)[a] =', dict(c.local_gradients)[a])
print('dict(c.local_gradients)[b] =', dict(c.local_gradients)[b])
```

```
dict(d.local_gradients)[a] = 7
dict(d.local_gradients)[c] = 4
dict(c.local_gradients)[a] = 1
dict(c.local_gradients)[b] = 1
```
    

We saw these in our example above as:

$ \frac{\partial \bar{d} }{\partial a}   = c = 7$

$ \frac{\partial \bar{d} }{\partial c}   = a = 4$

$ \frac{\partial \bar{c} }{\partial a}   = 1$

$ \frac{\partial \bar{c} }{\partial b}   = 1$


---

### A few improvements

Let's:
- Add a couple more functions.
- Enable the use of operators, such as +, *, - ... using Python's dunder (double underscore) methods.


```python
class Variable:
    def __init__(self, value, local_gradients=[]):
        self.value = value
        self.local_gradients = local_gradients
    
    def __add__(self, other):
        return add(self, other)
    
    def __mul__(self, other):
        return mul(self, other)
    
    def __sub__(self, other):
        return add(self, neg(other))

    def __truediv__(self, other):
        return mul(self, inv(other))
    
def add(a, b):
    value = a.value + b.value    
    local_gradients = (
        (a, 1),
        (b, 1)
    )
    return Variable(value, local_gradients)

def mul(a, b):
    value = a.value * b.value
    local_gradients = (
        (a, b.value),
        (b, a.value)
    )
    return Variable(value, local_gradients)

def neg(a):
    value = -1 * a.value
    local_gradients = (
        (a, -1),
    )
    return Variable(value, local_gradients)

def inv(a):
    value = 1. / a.value
    local_gradients = (
        (a, -1 / a.value**2),
    )
    return Variable(value, local_gradients)     
```

### Some more examples

We can get the gradients of arbitrary functions made from the functions we've added to the framework. E.g.


```python
a = Variable(230.3)
b = Variable(33.2)

def f(a, b):
    return (a / b - a) * (b / a + a + b) * (a - b)

y = f(a, b)

gradients = get_gradients(y)

print("The partial derivative of y with respect to a =", gradients[a])
print("The partial derivative of y with respect to b =", gradients[b])
```

```
The partial derivative of y with respect to a = -153284.83150602411
The partial derivative of y with respect to b = 3815.0389441500993
```    

We can use numerical estimates to check that we're getting correct results:

```python
delta = Variable(1e-8)
numerical_grad_a = (f(a + delta, b) - f(a, b)) / delta
numerical_grad_b = (f(a, b + delta) - f(a, b)) / delta
print("The numerical estimate for a =", numerical_grad_a.value)
print("The numerical estimate for b =", numerical_grad_b.value)
```

```
The numerical estimate for a = -153284.89243984222
The numerical estimate for b = 3815.069794654846
``` 

### It's easy to add more functions

You just need to be able to define the local derivatives.


```python
import numpy as np

def sin(a):
    value = np.sin(a.value)
    local_gradients = (
        (a, np.cos(a.value)),
    )
    return Variable(value, local_gradients)

def exp(a):
    value = np.exp(a.value)
    local_gradients = (
        (a, value),
    )
    return Variable(value, local_gradients)
    
def log(a):
    value = np.log(a.value)
    local_gradients = (
        (a, 1. / a.value),
    )
    return Variable(value, local_gradients)
```

Let's check that these work:


```python
a = Variable(43)
b = Variable(3)
c = Variable(2)

def f(a, b, c):
    f = sin(a * b) + exp(c - (a / b))
    return log(f * f) * c

y = f(a, b, c)

gradients = get_gradients(y)

print("The partial derivative of y with respect to a =", gradients[a])
print("The partial derivative of y with respect to b =", gradients[b])
print("The partial derivative of y with respect to c =", gradients[c])
```

```
The partial derivative of y with respect to a = 60.85353612046653
The partial derivative of y with respect to b = 872.2331479536114
The partial derivative of y with respect to c = -3.2853671032530305
``` 


```python
delta = Variable(1e-8)
numerical_grad_a = (f(a + delta, b, c) - f(a, b, c)) / delta
numerical_grad_b = (f(a, b + delta, c) - f(a, b, c)) / delta
numerical_grad_c = (f(a, b, c + delta) - f(a, b, c)) / delta

print("The numerical estimate for a =", numerical_grad_a.value)
print("The numerical estimate for b =", numerical_grad_b.value)
print("The numerical estimate for c =", numerical_grad_c.value)
```

    The numerical estimate for a = 60.85352186602222
    The numerical estimate for b = 872.232160009645
    The numerical estimate for c = -3.285367089489455
    

**That's the end of our minimal autodiff implementation!**

Of course there's various features missing, such as:
- Vectorisation.
- Placeholder variables.
- Nth derivatives.
- Optimisations.
- All the other amazing stuff deep learning / autodiff frameworks can do.

The most fruitful addition to our minimal framework would be vectorisation.

---

## A naive vectorisation

Let's look at a computationally inefficient, but easy to implement, method of vectorising our autodiff framework.

The approach is:

- Put our `Variable` objects from above into NumPy arrays
- We can then use NumPy operations
- That's it..

```python
import numpy as np

# convert NumPy array into array of Variable objects:
to_var = np.vectorize(lambda x : Variable(x))

# get values from array of Variable objects:
to_vals = np.vectorize(lambda variable : variable.value)
```

### A single linear layer neural network (fitting noise to noise):


```python
import matplotlib.pyplot as plt
np.random.seed(0)

def update_weights(weights, gradients, lrate):
    for index, weight in np.ndenumerate(weights):
        weights[index].value -= lrate * gradients[weight]

input_size = 50
output_size = 10
lrate = 0.001

x = to_var(np.random.random(input_size))
y_true = to_var(np.random.random(output_size))
weights = to_var(np.random.random((input_size, output_size)))

loss_vals = []
for i in range(100):
    y_pred = np.dot(x, weights)
    loss = np.sum((y_true - y_pred) * (y_true - y_pred))
    loss_vals.append(loss.value)
    gradients = get_gradients(loss)
    update_weights(weights, gradients, lrate)

plt.plot(loss_vals)
plt.xlabel("Time step")
plt.ylabel("Loss")
plt.title("Single linear layer learning")
plt.show()
```

<p align="center">
<img 
    src="/assets/posts/autodiff/loss.png" 
    alt="Plot of the loss of the linear layer."
/>
</p>

Coming soon: part two, where we look at how to vectorize our minimal framework more efficiency.

References:
- Hands-On Machine Learning with Scikit-Learn and TensorFlow. A Géron, A. 2017
- [https://en.wikipedia.org/wiki/Automatic_differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)

